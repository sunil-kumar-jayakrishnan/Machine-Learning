{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69be640-2aa7-40be-bc65-26578fa55858",
   "metadata": {},
   "source": [
    "ASSIGNMENT - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac6c65-1b9c-4d7f-a335-790df8132406",
   "metadata": {},
   "source": [
    "Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58a324-075a-4322-9aff-eb3eb34934d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6cf3f-9492-46a0-863b-0fb91533eef1",
   "metadata": {},
   "source": [
    "To find : P (C sunny/ a cone of ice-cream) and P (rainy /a cup of hot coffee)\n",
    "Given the occurrence of each word is Independent\n",
    "P (a cone of ice cream) = P(a) (cone) P(of) P(ice) P(cream)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f4b2b-0858-4219-8642-c4791ecdd9f1",
   "metadata": {},
   "source": [
    "a)\n",
    "P(Sunny/a cone of ice cream)\n",
    "Using Bayes theorem :P(A/B) = (PB/A)*P(A)/P(B)\n",
    "we can find\n",
    "P (Sunny/a cone of Ice-cream) = P (a cone of Ice-cream/sunny) * (P (sunny)/ P(a cone of Ice-cream)) Also, since the words are independent:\n",
    "we want to classify the tags with higher probability, so ignoring the denominator\n",
    "therefore: \n",
    "P (Sunny/a cone of Ice-cream) = P(a/sunny) * P (Cone/sunny) * P (of/ sunny) * P(Ice/ sunny)* P (Cream/sunny)* P (Sunny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded6f9b-510b-4b24-8b64-deaf1b733147",
   "metadata": {},
   "source": [
    "b)\n",
    "Similarly: \n",
    "P (rainy/a cup of hot coffee) = P(a/rainy).P(cup/rainy).P(of/rainy).P(hot/rainy).P(coffee/rainy).P(rainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb90c3-d4b1-43b3-bbf8-ff3d935bcc56",
   "metadata": {},
   "source": [
    "question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba3d7a-3946-4658-bb6e-5c6582f42a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import classificationMethod\n",
    "import math\n",
    "\n",
    "class NaiveBayesClassifier(classificationMethod.ClassificationMethod):\n",
    "  \"\"\"\n",
    "  See the project description for the specifications of the Naive Bayes classifier.\n",
    "  \n",
    "  Note that the variable 'datum' in this code refers to a counter of features\n",
    "  (not to a raw samples.Datum).\n",
    "  \"\"\"\n",
    "  def __init__(self, legalLabels):\n",
    "    self.legalLabels = legalLabels\n",
    "    self.type = \"naivebayes\"\n",
    "    self.k = 1 # this is the smoothing parameter, ** use it in your train method **\n",
    "    self.automaticTuning = False # Look at this flag to decide whether to choose k automatically ** use this in your train method **\n",
    "    \n",
    "  def setSmoothing(self, k):\n",
    "    \"\"\"\n",
    "    This is used by the main method to change the smoothing parameter before training.\n",
    "    Do not modify this method.\n",
    "    \"\"\"\n",
    "    self.k = k\n",
    "\n",
    "  def train(self, trainingData, trainingLabels, validationData, validationLabels):\n",
    "    \"\"\"\n",
    "    Outside shell to call your method. Do not modify this method.\n",
    "    \"\"\"  \n",
    "      \n",
    "    self.features = list(trainingData[0].keys()) # this could be useful for your code later...\n",
    "    \n",
    "    if (self.automaticTuning):\n",
    "        kgrid = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50]\n",
    "    else:\n",
    "        kgrid = [self.k]\n",
    "        \n",
    "    self.trainAndTune(trainingData, trainingLabels, validationData, validationLabels, kgrid)\n",
    "      \n",
    "  def trainAndTune(self, trainingData, trainingLabels, validationData, validationLabels, kgrid):\n",
    "    \"\"\"\n",
    "    Trains the classifier by collecting counts over the training data, and\n",
    "    stores the Laplace smoothed estimates so that they can be used to classify.\n",
    "    Evaluate each value of k in kgrid to choose the smoothing parameter \n",
    "    that gives the best accuracy on the held-out validationData.\n",
    "    \n",
    "    trainingData and validationData are lists of feature Counters.  The corresponding\n",
    "    label lists contain the correct label for each datum.\n",
    "    \n",
    "    To get the list of all possible features or labels, use self.features and \n",
    "    self.legalLabels.\n",
    "    \"\"\"\n",
    "\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    P = util.Counter()\n",
    "    for l in trainingLabels:\n",
    "        P[l] += 1\n",
    "    P.normalize()\n",
    "    self.P = P\n",
    "    \n",
    "    # Initialize stuff\n",
    "    counts = {}\n",
    "    totals = {}\n",
    "    for f in self.features:\n",
    "        counts[f] = {0: util.Counter(), 1: util.Counter()}\n",
    "        totals[f] = util.Counter()\n",
    "                 \n",
    "    # Calculate totals and counts\n",
    "    for i, datum in enumerate(trainingData):\n",
    "        y = trainingLabels[i]\n",
    "        for f, value in list(datum.items()):\n",
    "            counts[f][value][y] += 1.0\n",
    "            totals[f][y] += 1.0 \n",
    "            \n",
    "    bestConditionals = {}\n",
    "    # bestAccuracy = None\n",
    "    bestAccuracy = 0\n",
    "    # Evaluate each k, and use the one that yields the best accuracy\n",
    "    for k in kgrid or [0.0]:\n",
    "        correct = 0\n",
    "        conditionals = {}            \n",
    "        for f in self.features:\n",
    "            conditionals[f] = {0: util.Counter(), 1: util.Counter()}\n",
    "            \n",
    "        # Run Laplace smoothing\n",
    "        for f in self.features:\n",
    "            for value in [0, 1]:\n",
    "                for y in self.legalLabels:\n",
    "                    conditionals[f][value][y] = (counts[f][value][y] + k) / (totals[f][y] + k * 2)\n",
    "            \n",
    "        # Check the accuracy associated with this k\n",
    "        self.conditionals = conditionals              \n",
    "        guesses = self.classify(validationData)\n",
    "        for i, guess in enumerate(guesses):\n",
    "            correct += (validationLabels[i] == guess and 1.0 or 0.0)\n",
    "        accuracy = correct / len(guesses)\n",
    "        \n",
    "        # Keep the best k so far\n",
    "        # if accuracy > bestAccuracy or bestAccuracy is None:\n",
    "        if accuracy > bestAccuracy:\n",
    "          bestAccuracy = accuracy\n",
    "          bestConditionals = conditionals\n",
    "          self.k = k\n",
    "            \n",
    "    self.conditionals = bestConditionals\n",
    "    # util.raiseNotDefined()\n",
    "        \n",
    "  def classify(self, testData):\n",
    "    \"\"\"\n",
    "    Classify the data based on the posterior distribution over labels.\n",
    "    \n",
    "    You shouldn't modify this method.\n",
    "    \"\"\"\n",
    "    guesses = []\n",
    "    self.posteriors = [] # Log posteriors are stored for later data analysis (autograder).\n",
    "    for datum in testData:\n",
    "      posterior = self.calculateLogJointProbabilities(datum)\n",
    "      guesses.append(posterior.argMax())\n",
    "      self.posteriors.append(posterior)\n",
    "    return guesses\n",
    "      \n",
    "  def calculateLogJointProbabilities(self, datum):\n",
    "    \"\"\"\n",
    "    Returns the log-joint distribution over legal labels and the datum.\n",
    "    Each log-probability should be stored in the log-joint counter, e.g.    \n",
    "    logJoint[3] = <Estimate of log( P(Label = 3, datum) )>\n",
    "    \"\"\"\n",
    "    logJoint = util.Counter()\n",
    "    \n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    logJoint = util.Counter()\n",
    "    evidence = datum.items()\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    for y in self.legalLabels:\n",
    "      logJoint[y] = math.log(self.P[y])\n",
    "      for f in self.conditionals:\n",
    "        prob = self.conditionals[f][datum[f]][y]\n",
    "        logJoint[y] += (prob and math.log(prob) or 0.0)\n",
    "    # util.raiseNotDefined()\n",
    "    \n",
    "    return logJoint\n",
    "  \n",
    "  def findHighOddsFeatures(self, label1, label2):\n",
    "    \"\"\"\n",
    "    Returns the 100 best features for the odds ratio:\n",
    "            P(feature=1 | label1)/P(feature=1 | label2) \n",
    "    \"\"\"\n",
    "    featuresOdds = []\n",
    "        \n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    for y in self.legalLabels:\n",
    "      logJoint[y] = math.log(self.P[y])\n",
    "      for f in self.conditionals:\n",
    "          prob = self.conditionals[f][datum[f]][y]\n",
    "          logJoint[y] += (prob and math.log(prob) or 0.0)\n",
    "    # util.raiseNotDefined()\n",
    "\n",
    "    return featuresOdds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7f654-48c5-44bd-92f1-100f5bc17938",
   "metadata": {},
   "source": [
    "Output is given in the Q2 folder\n",
    "\n",
    "References\n",
    "ML references\n",
    "https://www.youtube.com/watch?v=FgaM-TzT7qk&feature=emb_imp_woyt\n",
    "\n",
    "qn.2\n",
    "For converting from Python 2 to 3\n",
    "https://docs.python.org/3/library/2to3.html#module-lib2to3\n",
    "\n",
    "Code reference for dataClassifier\n",
    "https://github.com/anthony-niklas/cs188/blob/341f854af50863f6f30e09ca32910ee3025ec5b2/p5/dataClassifier.py\n",
    "\n",
    "Code reference for naiveBayes\n",
    "https://github.com/anthony-niklas/cs188/blob/master/p5/naiveBayes.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
